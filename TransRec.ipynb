{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "csv = np.loadtxt('./ratings_Video_Games.csv', dtype={'names': ('user', 'item', 'rating', 'time'),\n",
    "                                                        'formats': ('S14', 'S10', 'f', 'i')}, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = np.array(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1324753,) (b'AB9S9279OZ3QO', b'0078764343', 5., 1373155200)\n"
     ]
    }
   ],
   "source": [
    "print(csv.shape, csv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class StringMap:\n",
    "    def __init__(self):\n",
    "        self.mi = {}\n",
    "        self.ms = {}\n",
    "        self.counter = 0\n",
    "        \n",
    "    def s2i(self, s):\n",
    "        if not s in self.ms:\n",
    "            self.mi[self.counter] = s\n",
    "            self.ms[s] = self.counter\n",
    "            self.counter += 1\n",
    "        return self.ms[s]\n",
    "    \n",
    "    def i2s(self, i):\n",
    "        return self.mi[i]\n",
    "\n",
    "def csv_to_time_series(csv):\n",
    "    user_count = defaultdict(int)\n",
    "    item_count = defaultdict(int)\n",
    "    for i in range(csv.shape[0]):\n",
    "        line = csv[i]\n",
    "        user_count[line[0]] += 1\n",
    "        item_count[line[1]] += 1\n",
    "\n",
    "    user = StringMap()\n",
    "    item = StringMap()\n",
    "\n",
    "    useritem = {}\n",
    "    for i in range(csv.shape[0]):\n",
    "        line = csv[i]\n",
    "        userid = user.s2i(line[0])\n",
    "        itemid = item.s2i(line[1])\n",
    "        ts = line[3]\n",
    "        if userid not in useritem:\n",
    "            useritem[userid] = []\n",
    "        useritem[userid].append((ts, itemid))\n",
    "        \n",
    "    return user, item, useritem\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_to_triple(user_map, item_map, user_item):\n",
    "    test_set = []\n",
    "    validation_set = []\n",
    "    training_set = []\n",
    "    for x in useritem:\n",
    "        useritem[x].sort()\n",
    "        if len(useritem[x]) < 4:\n",
    "            continue\n",
    "        test_set.append((x, useritem[x][-2][1], useritem[x][-1][1]))\n",
    "        validation_set.append((x, useritem[x][-3][1], useritem[x][-2][1]))\n",
    "        for i in range(len(useritem[x]) - 3):\n",
    "            training_set.append((x, useritem[x][i][1], useritem[x][i+1][1]))\n",
    "\n",
    "    final_user_map = StringMap()\n",
    "    final_item_map = StringMap()\n",
    "\n",
    "    def finale_map(old_user_map, old_item_map, user_map, item_map, data_set):\n",
    "        result = []\n",
    "        for u, i, j in data_set:\n",
    "            userid = user_map.s2i(old_user_map.i2s(u))\n",
    "            iid = item_map.s2i(old_item_map.i2s(i))\n",
    "            jid = item_map.s2i(old_item_map.i2s(j))\n",
    "            result.append((userid, iid, jid))\n",
    "        return result\n",
    "\n",
    "    training_data = np.array(finale_map(user_map, item_map, final_user_map, final_item_map, training_set))\n",
    "    validation_data = np.array(finale_map(user_map, item_map, final_user_map, final_item_map, validation_set))\n",
    "    test_data = np.array(finale_map(user_map, item_map, final_user_map, final_item_map, test_set))\n",
    "    return final_user_map, final_item_map, training_data, validation_data, test_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_map, item_map, user_item = csv_to_time_series(csv)\n",
    "user_map, item_map, training_data, validation_data, test_data = time_series_to_triple(user_map, item_map, user_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     1]\n",
      " [    0     1     2]\n",
      " [    0     2     3]\n",
      " ...\n",
      " [47916 29293 32176]\n",
      " [47916 32176 32166]\n",
      " [47917 32418 32419]]\n"
     ]
    }
   ],
   "source": [
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  297  3412  3413 26185]\n",
      " [ 7503  4548  4425  9067]\n",
      " [36825  9021   409 35279]\n",
      " ...\n",
      " [  699  3020  4064 11014]\n",
      " [ 8290  5790  6989 21421]\n",
      " [23269 10402  2845 19084]]\n",
      "223813 36303 47918\n"
     ]
    }
   ],
   "source": [
    "n_item = item_map.counter\n",
    "n_user = user_map.counter\n",
    "n_record = training_data.shape[0]\n",
    "j_neg = np.random.randint(0, n_item, (n_record, 1))\n",
    "training_data = np.concatenate((training_data, j_neg), axis=1)\n",
    "np.random.shuffle(training_data)\n",
    "print(training_data)\n",
    "print(n_record, n_item, n_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.4 |Anaconda, Inc.| (default, Mar 12 2018, 20:05:31) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "1.5.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tqdm import tnrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trans-E Model for Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransE(object):\n",
    "    def __init__(self, n_item, n_user, embedding_dim, batch_size, beta, learning_rate):\n",
    "        self.n_item = n_item\n",
    "        self.n_user = n_user\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.beta = beta\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.global_step = tf.Variable(initial_value=0, trainable=False, name='global_step')\n",
    "        self.quadruple = tf.placeholder(tf.int32, [None, 4])\n",
    "        \n",
    "        with tf.variable_scope(\"embedding\"):\n",
    "            self.item_embedding = tf.get_variable(name = \"item_embedding\", shape = [self.n_item, self.embedding_dim], initializer = tf.contrib.layers.xavier_initializer(uniform = False))\n",
    "            self.user_embedding = tf.get_variable(name = \"user_embedding\", shape = [self.n_user, self.embedding_dim], initializer = tf.zeros_initializer())\n",
    "            self.item_bias = tf.get_variable(name = \"item_bias\", shape = [self.n_item], initializer = tf.zeros_initializer())\n",
    "\n",
    "    def build_graph(self):\n",
    "        # Normalize item embeddings into \\Omega space\n",
    "        with tf.name_scope('normalization'):\n",
    "            self.item_embedding = tf.nn.l2_normalize(self.item_embedding, dim=1)\n",
    "            \n",
    "        with tf.name_scope('training'):\n",
    "            dist_positive, dist_negative, bias_positive, bias_negative = self.inference(self.quadruple)\n",
    "            self.loss = self.loss_function(dist_positive, dist_negative, bias_positive, bias_negative)\n",
    "            \n",
    "            #tf.summary.scalar(name=self.loss.op.name, tensor=self.loss)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "            self.train_op = optimizer.minimize(self.loss, global_step=self.global_step)\n",
    "            #self.merge = tf.summary.merge_all()\n",
    "            \n",
    "    def loss_function(self, dist_positive, dist_negative, bias_positive, bias_negative):\n",
    "        with tf.name_scope('loss_function'):\n",
    "            # L2 distance\n",
    "            prob_positive = bias_positive - tf.reduce_sum(dist_positive ** 2, axis=1)\n",
    "            prob_negative = bias_negative - tf.reduce_sum(dist_negative ** 2, axis=1)\n",
    "            \n",
    "            # L2 regularization\n",
    "            regularizer = tf.nn.l2_loss(self.item_embedding) + tf.nn.l2_loss(self.user_embedding)\n",
    "            \n",
    "            # S-BPR loss\n",
    "            loss = -tf.reduce_sum(tf.log(tf.nn.sigmoid(prob_positive - prob_negative)), name='SBPR_loss') + self.beta * regularizer\n",
    "        return loss\n",
    "    \n",
    "    # Modify this method to apply Trans-E\n",
    "    def inference(self, quadruple):\n",
    "        # quadruple (user, i, j, j')\n",
    "        with tf.name_scope('embedding_lookup'):\n",
    "            i = tf.nn.embedding_lookup(self.item_embedding, quadruple[:, 1])\n",
    "            j = tf.nn.embedding_lookup(self.item_embedding, quadruple[:, 2])\n",
    "            j_neg = tf.nn.embedding_lookup(self.item_embedding, quadruple[:, 3])\n",
    "            u = tf.nn.embedding_lookup(self.user_embedding, quadruple[:, 0])\n",
    "            \n",
    "            bias_positive = tf.nn.embedding_lookup(self.item_bias, quadruple[:, 2])\n",
    "            bias_negative = tf.nn.embedding_lookup(self.item_bias, quadruple[:, 3])\n",
    "            \n",
    "        with tf.name_scope('link'):\n",
    "            dist_positive = i + u - j # -> 0\n",
    "            dist_negative = i + u - j_neg # -> +oo\n",
    "        return dist_positive, dist_negative, bias_positive, bias_negative\n",
    "    \n",
    "    def train(self, training_data, n_epochs, session, summary_writer):\n",
    "        for epoch in tnrange(n_epochs):\n",
    "            epoch_loss = 0\n",
    "            n_batch = training_data.shape[0] // self.batch_size\n",
    "            t = tnrange(n_batch, leave=False)\n",
    "            for i in t:\n",
    "                quadruple = training_data[i : i+self.batch_size]\n",
    "                batch_loss, _ = session.run(fetches=[self.loss, self.train_op],\n",
    "                                                     feed_dict={self.quadruple: quadruple})\n",
    "                # summary_writer.add_summary(summary, global_step=self.global_step.eval(session=session))\n",
    "                epoch_loss += batch_loss\n",
    "                t.set_description(\"%.3f\" % batch_loss)\n",
    "                t.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model = TransE(n_item=n_item,\n",
    "       n_user=n_user,\n",
    "       batch_size=50,\n",
    "       learning_rate=1e-5,\n",
    "       beta=1e-7,\n",
    "       embedding_dim=20)\n",
    "model.build_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4113733f71b645b39394593c4a932785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4476), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4476), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4476), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdfc9670861145e5a91072eedbdaaa4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4476), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-c19733a068a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-125-aec9f5c1dbe7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_data, n_epochs, session, summary_writer)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mquadruple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 batch_loss, _ = session.run(fetches=[self.loss, self.train_op],\n\u001b[0;32m---> 70\u001b[0;31m                                                      feed_dict={self.quadruple: quadruple})\n\u001b[0m\u001b[1;32m     71\u001b[0m                 \u001b[0;31m# summary_writer.add_summary(summary, global_step=self.global_step.eval(session=session))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "model.train(training_data, 10, sess, None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trans-H Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransH(object):\n",
    "    def __init__(self, n_item, n_user, embedding_dim, batch_size, beta, learning_rate):\n",
    "        self.n_item = n_item\n",
    "        self.n_user = n_user\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.beta = beta\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.global_step = tf.Variable(initial_value=0, trainable=False, name='global_step')\n",
    "        self.quadruple = tf.placeholder(tf.int32, [None, 4])\n",
    "        \n",
    "        with tf.variable_scope(\"embedding\"):\n",
    "            self.item_embedding = tf.get_variable(name = \"item_embedding\", shape = [self.n_item, self.embedding_dim], initializer = tf.contrib.layers.xavier_initializer(uniform = False))\n",
    "            self.user_embedding = tf.get_variable(name = \"user_embedding\", shape = [self.n_user, self.embedding_dim], initializer = tf.contrib.layers.xavier_initializer(uniform = False))\n",
    "            self.normal_vector = tf.get_variable(name = \"normal_vector\", shape = [self.n_user, self.embedding_dim], initializer = tf.contrib.layers.xavier_initializer(uniform = False))\n",
    "            self.item_bias = tf.get_variable(name = \"item_bias\", shape = [self.n_item], initializer = tf.zeros_initializer())\n",
    "\n",
    "    def build_graph(self):\n",
    "        # Normalize item embeddings into \\Omega space\n",
    "        with tf.name_scope('normalization'):\n",
    "            self.item_embedding = tf.nn.l2_normalize(self.item_embedding, dim=1)\n",
    "            self.normal_vector = tf.nn.l2_normalize(self.normal_vector, dim=1)\n",
    "            \n",
    "        with tf.name_scope('training'):\n",
    "            dist_positive, dist_negative, bias_positive, bias_negative = self.inference(self.quadruple)\n",
    "            self.loss = self.loss_function(dist_positive, dist_negative, bias_positive, bias_negative)\n",
    "            \n",
    "            #tf.summary.scalar(name=self.loss.op.name, tensor=self.loss)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "            self.train_op = optimizer.minimize(self.loss, global_step=self.global_step)\n",
    "            #self.merge = tf.summary.merge_all()\n",
    "            \n",
    "    def loss_function(self, dist_positive, dist_negative, bias_positive, bias_negative):\n",
    "        with tf.name_scope('loss_function'):\n",
    "            # L2 distance\n",
    "            prob_positive = bias_positive - tf.reduce_sum(dist_positive ** 2, axis=1)\n",
    "            prob_negative = bias_negative - tf.reduce_sum(dist_negative ** 2, axis=1)\n",
    "            \n",
    "            # L2 regularization\n",
    "            regularizer = tf.nn.l2_loss(self.item_embedding) + tf.nn.l2_loss(self.user_embedding)\n",
    "            \n",
    "            # S-BPR loss\n",
    "            loss = -tf.reduce_sum(tf.log(tf.nn.sigmoid(prob_positive - prob_negative)), name='SBPR_loss') + self.beta * regularizer\n",
    "        return loss\n",
    "    \n",
    "    # Modify this method to apply Trans-E\n",
    "    def inference(self, quadruple):\n",
    "        # quadruple (user, i, j, j')\n",
    "        def projection(embedded, norm):\n",
    "            return embedded - tf.reduce_sum(embedded * norm, 1, keepdims = True) * norm\n",
    "        \n",
    "        with tf.name_scope('embedding_lookup'):\n",
    "            i = tf.nn.embedding_lookup(self.item_embedding, quadruple[:, 1])\n",
    "            j = tf.nn.embedding_lookup(self.item_embedding, quadruple[:, 2])\n",
    "            j_neg = tf.nn.embedding_lookup(self.item_embedding, quadruple[:, 3])\n",
    "            u = tf.nn.embedding_lookup(self.user_embedding, quadruple[:, 0])\n",
    "            norm = tf.nn.embedding_lookup(self.normal_vector, quadruple[:, 0])\n",
    "            \n",
    "            i, j, j_neg = projection(i, norm), projection(j, norm), projection(j_neg, norm)\n",
    "            \n",
    "            bias_positive = tf.nn.embedding_lookup(self.item_bias, quadruple[:, 2])\n",
    "            bias_negative = tf.nn.embedding_lookup(self.item_bias, quadruple[:, 3])\n",
    "            \n",
    "        with tf.name_scope('link'):\n",
    "            dist_positive = i + u - j # -> 0\n",
    "            dist_negative = i + u - j_neg # -> +oo\n",
    "        return dist_positive, dist_negative, bias_positive, bias_negative\n",
    "    \n",
    "    def train(self, training_data, n_epochs, session, summary_writer):\n",
    "        for epoch in tnrange(n_epochs):\n",
    "            epoch_loss = 0\n",
    "            n_batch = training_data.shape[0] // self.batch_size\n",
    "            t = tnrange(n_batch, leave=False)\n",
    "            for i in t:\n",
    "                quadruple = training_data[i : i+self.batch_size]\n",
    "                batch_loss, _ = session.run(fetches=[self.loss, self.train_op],\n",
    "                                                     feed_dict={self.quadruple: quadruple})\n",
    "                # summary_writer.add_summary(summary, global_step=self.global_step.eval(session=session))\n",
    "                epoch_loss += batch_loss\n",
    "                t.set_description(\"%.3f\" % batch_loss)\n",
    "                t.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-135-b285e7d5fe8c>:51: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5614ec18f746f1ac41ff718dbe505e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f762f206aa48a1966d6416d2202368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4476), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-5b510f7739d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-135-b285e7d5fe8c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_data, n_epochs, session, summary_writer)\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mquadruple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 batch_loss, _ = session.run(fetches=[self.loss, self.train_op],\n\u001b[0;32m---> 78\u001b[0;31m                                                      feed_dict={self.quadruple: quadruple})\n\u001b[0m\u001b[1;32m     79\u001b[0m                 \u001b[0;31m# summary_writer.add_summary(summary, global_step=self.global_step.eval(session=session))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "model = TransH(n_item=n_item,\n",
    "       n_user=n_user,\n",
    "       batch_size=50,\n",
    "       learning_rate=1e-5,\n",
    "       beta=1e-7,\n",
    "       embedding_dim=20)\n",
    "model.build_graph()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "model.train(training_data, 10, sess, None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trans-R Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
